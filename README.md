
# trojai-literature

The list below contains arXiv articles that have been identified as being relevant to the subject of adversarial examples in neural networks.
These articles were identified using a [flair](https://github.com/flairNLP/flair) embedding created from the arXiv CS subset; details will be provided later.


1. [A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks](https://arxiv.org/search/?query=%27A+Baseline+for+Detecting+Misclassified+and+Out-of-Distribution+Examples+in+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [A Boundary Tilting Persepective on the Phenomenon of Adversarial Examples](https://arxiv.org/search/?query=%27A+Boundary+Tilting+Persepective+on+the+Phenomenon+of+Adversarial+Examples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [A Learning and Masking Approach to Secure Learning](https://arxiv.org/search/?query=%27A+Learning+and+Masking+Approach+to+Secure+Learning%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks](https://arxiv.org/search/?query=%27A+Simple+Unified+Framework+for+Detecting+Out-of-Distribution+Samples+and+Adversarial+Attacks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [A Unified View of Piecewise Linear Neural Network Verification](https://arxiv.org/search/?query=%27A+Unified+View+of+Piecewise+Linear+Neural+Network+Verification%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [A general metric for identifying adversarial images](https://arxiv.org/search/?query=%27A+general+metric+for+identifying+adversarial+images%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [A3T: Adversarially Augmented Adversarial Training](https://arxiv.org/search/?query=%27A3T:+Adversarially+Augmented+Adversarial+Training%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [APE-GAN: Adversarial Perturbation Elimination with GAN](https://arxiv.org/search/?query=%27APE-GAN:+Adversarial+Perturbation+Elimination+with+GAN%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [ASP:A Fast Adversarial Attack Example Generation Framework based on Adversarial Saliency Prediction](https://arxiv.org/search/?query=%27ASP:A+Fast+Adversarial+Attack+Example+Generation+Framework+based+on+Adversarial+Saliency+Prediction%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adaptive Spatial Steganography Based on Probability-Controlled Adversarial Examples](https://arxiv.org/search/?query=%27Adaptive+Spatial+Steganography+Based+on+Probability-Controlled+Adversarial+Examples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network](https://arxiv.org/search/?query=%27Adv-BNN:+Improved+Adversarial+Defense+through+Robust+Bayesian+Neural+Network%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarial Attack on Graph Structured Data](https://arxiv.org/search/?query=%27Adversarial+Attack+on+Graph+Structured+Data%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarial Attacks on Face Detectors using Neural Net based Constrained Optimization](https://arxiv.org/search/?query=%27Adversarial+Attacks+on+Face+Detectors+using+Neural+Net+based+Constrained+Optimization%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarial Attacks on Neural Network Policies](https://arxiv.org/search/?query=%27Adversarial+Attacks+on+Neural+Network+Policies%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarial Attacks on Neural Networks for Graph Data](https://arxiv.org/search/?query=%27Adversarial+Attacks+on+Neural+Networks+for+Graph+Data%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarial Deep Learning for Robust Detection of Binary Encoded Malware](https://arxiv.org/search/?query=%27Adversarial+Deep+Learning+for+Robust+Detection+of+Binary+Encoded+Malware%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarial Defense based on Structure-to-Signal Autoencoders](https://arxiv.org/search/?query=%27Adversarial+Defense+based+on+Structure-to-Signal+Autoencoders%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarial Defense via Data Dependent Activation Function and Total Variation Minimization](https://arxiv.org/search/?query=%27Adversarial+Defense+via+Data+Dependent+Activation+Function+and+Total+Variation+Minimization%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarial Distillation of Bayesian Neural Network Posteriors](https://arxiv.org/search/?query=%27Adversarial+Distillation+of+Bayesian+Neural+Network+Posteriors%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarial Diversity and Hard Positive Generation](https://arxiv.org/search/?query=%27Adversarial+Diversity+and+Hard+Positive+Generation%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarial Example Defenses: Ensembles of Weak Defenses are not Strong](https://arxiv.org/search/?query=%27Adversarial+Example+Defenses:+Ensembles+of+Weak+Defenses+are+not+Strong%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods](https://arxiv.org/search/?query=%27Adversarial+Examples+Are+Not+Easily+Detected:+Bypassing+Ten+Detection+Methods%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarial Examples Detection in Deep Networks with Convolutional Filter Statistics](https://arxiv.org/search/?query=%27Adversarial+Examples+Detection+in+Deep+Networks+with+Convolutional+Filter+Statistics%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarial Examples for Semantic Image Segmentation](https://arxiv.org/search/?query=%27Adversarial+Examples+for+Semantic+Image+Segmentation%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarial Examples: Attacks and Defenses for Deep Learning](https://arxiv.org/search/?query=%27Adversarial+Examples:+Attacks+and+Defenses+for+Deep+Learning%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarial Examples: Attacks on Machine Learning-based Malware Visualization Detection Methods](https://arxiv.org/search/?query=%27Adversarial+Examples:+Attacks+on+Machine+Learning-based+Malware+Visualization+Detection+Methods%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarial Examples: Opportunities and Challenges](https://arxiv.org/search/?query=%27Adversarial+Examples:+Opportunities+and+Challenges%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarial Generative Nets: Neural Network Attacks on State-of-the-Art Face Recognition](https://arxiv.org/search/?query=%27Adversarial+Generative+Nets:+Neural+Network+Attacks+on+State-of-the-Art+Face+Recognition%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarial Malware Binaries: Evading Deep Learning for Malware Detection in Executables](https://arxiv.org/search/?query=%27Adversarial+Malware+Binaries:+Evading+Deep+Learning+for+Malware+Detection+in+Executables%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarial Noise Layer: Regularize Neural Network By Adding Noise](https://arxiv.org/search/?query=%27Adversarial+Noise+Layer:+Regularize+Neural+Network+By+Adding+Noise%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarial Perturbations Against Deep Neural Networks for Malware Classification](https://arxiv.org/search/?query=%27Adversarial+Perturbations+Against+Deep+Neural+Networks+for+Malware+Classification%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarial Phenomenon in the Eyes of Bayesian Deep Learning](https://arxiv.org/search/?query=%27Adversarial+Phenomenon+in+the+Eyes+of+Bayesian+Deep+Learning%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarial Reprogramming of Neural Networks](https://arxiv.org/search/?query=%27Adversarial+Reprogramming+of+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarial Reprogramming of Sequence Classification Neural Networks](https://arxiv.org/search/?query=%27Adversarial+Reprogramming+of+Sequence+Classification+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarial Robustness Toolbox v0](https://arxiv.org/search/?query=%27Adversarial+Robustness+Toolbox+v0%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarial Robustness: Softmax versus Openmax](https://arxiv.org/search/?query=%27Adversarial+Robustness:+Softmax+versus+Openmax%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarial Training for Probabilistic Spiking Neural Networks](https://arxiv.org/search/?query=%27Adversarial+Training+for+Probabilistic+Spiking+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarial Transformation Networks: Learning to Generate Adversarial Examples](https://arxiv.org/search/?query=%27Adversarial+Transformation+Networks:+Learning+to+Generate+Adversarial+Examples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarial and Clean Data Are Not Twins](https://arxiv.org/search/?query=%27Adversarial+and+Clean+Data+Are+Not+Twins%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarial-Playground: A Visualization Suite Showing How Adversarial Examples Fool Deep Learning](https://arxiv.org/search/?query=%27Adversarial-Playground:+A+Visualization+Suite+Showing+How+Adversarial+Examples+Fool+Deep+Learning%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarial-Playground: A Visualization Suite for Adversarial Sample Generation](https://arxiv.org/search/?query=%27Adversarial-Playground:+A+Visualization+Suite+for+Adversarial+Sample+Generation%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarially Regularising Neural NLI Models to Integrate Logical Background Knowledge](https://arxiv.org/search/?query=%27Adversarially+Regularising+Neural+NLI+Models+to+Integrate+Logical+Background+Knowledge%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversarially Robust Training through Structured Gradient Regularization](https://arxiv.org/search/?query=%27Adversarially+Robust+Training+through+Structured+Gradient+Regularization%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Adversary Resistant Deep Neural Networks with an Application to Malware Detection](https://arxiv.org/search/?query=%27Adversary+Resistant+Deep+Neural+Networks+with+an+Application+to+Malware+Detection%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [An ADMM-Based Universal Framework for Adversarial Attacks on Deep Neural Networks](https://arxiv.org/search/?query=%27An+ADMM-Based+Universal+Framework+for+Adversarial+Attacks+on+Deep+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [An Explainable Adversarial Robustness Metric for Deep Learning Neural Networks](https://arxiv.org/search/?query=%27An+Explainable+Adversarial+Robustness+Metric+for+Deep+Learning+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Are Accuracy and Robustness Correlated](https://arxiv.org/search/?query=%27Are+Accuracy+and+Robustness+Correlated%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Are Facial Attributes Adversarially Robust](https://arxiv.org/search/?query=%27Are+Facial+Attributes+Adversarially+Robust%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Are Generative Classifiers More Robust to Adversarial Attacks](https://arxiv.org/search/?query=%27Are+Generative+Classifiers+More+Robust+to+Adversarial+Attacks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Are You Tampering With My Data](https://arxiv.org/search/?query=%27Are+You+Tampering+With+My+Data%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Are adversarial examples inevitable](https://arxiv.org/search/?query=%27Are+adversarial+examples+inevitable%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Ask, Acquire, and Attack: Data-free UAP Generation using Class Impressions](https://arxiv.org/search/?query=%27Ask,+Acquire,+and+Attack:+Data-free+UAP+Generation+using+Class+Impressions%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Assessing Threat of Adversarial Examples on Deep Neural Networks](https://arxiv.org/search/?query=%27Assessing+Threat+of+Adversarial+Examples+on+Deep+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Attack and Defense of Dynamic Analysis-Based, Adversarial Neural Malware Classification Models](https://arxiv.org/search/?query=%27Attack+and+Defense+of+Dynamic+Analysis-Based,+Adversarial+Neural+Malware+Classification+Models%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Attacking Convolutional Neural Network using Differential Evolution](https://arxiv.org/search/?query=%27Attacking+Convolutional+Neural+Network+using+Differential+Evolution%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning](https://arxiv.org/search/?query=%27Attacking+Visual+Language+Grounding+with+Adversarial+Examples:+A+Case+Study+on+Neural+Image+Captioning%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [AutoZOOM: Autoencoder-based Zeroth Order Optimization Method for Attacking Black-box Neural Networks](https://arxiv.org/search/?query=%27AutoZOOM:+Autoencoder-based+Zeroth+Order+Optimization+Method+for+Attacking+Black-box+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Average Margin Regularization for Classifiers](https://arxiv.org/search/?query=%27Average+Margin+Regularization+for+Classifiers%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Backdoor Embedding in Convolutional Neural Network Models via Invisible Perturbation](https://arxiv.org/search/?query=%27Backdoor+Embedding+in+Convolutional+Neural+Network+Models+via+Invisible+Perturbation%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain](https://arxiv.org/search/?query=%27BadNets:+Identifying+Vulnerabilities+in+the+Machine+Learning+Model+Supply+Chain%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Biologically inspired protection of deep networks from adversarial attacks](https://arxiv.org/search/?query=%27Biologically+inspired+protection+of+deep+networks+from+adversarial+attacks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Black-Box Attacks against RNN based Malware Detection Algorithms](https://arxiv.org/search/?query=%27Black-Box+Attacks+against+RNN+based+Malware+Detection+Algorithms%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Black-box Adversarial Attacks with Limited Queries and Information](https://arxiv.org/search/?query=%27Black-box+Adversarial+Attacks+with+Limited+Queries+and+Information%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Boosting Adversarial Attacks with Momentum](https://arxiv.org/search/?query=%27Boosting+Adversarial+Attacks+with+Momentum%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Breaking Transferability of Adversarial Samples with Randomness](https://arxiv.org/search/?query=%27Breaking+Transferability+of+Adversarial+Samples+with+Randomness%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Building Robust Deep Neural Networks for Road Sign Detection](https://arxiv.org/search/?query=%27Building+Robust+Deep+Neural+Networks+for+Road+Sign+Detection%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Certified Defenses against Adversarial Examples](https://arxiv.org/search/?query=%27Certified+Defenses+against+Adversarial+Examples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Certified Robustness to Adversarial Examples with Differential Privacy](https://arxiv.org/search/?query=%27Certified+Robustness+to+Adversarial+Examples+with+Differential+Privacy%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Characterizing Adversarial Examples Based on Spatial Consistency Information for Semantic Segmentation](https://arxiv.org/search/?query=%27Characterizing+Adversarial+Examples+Based+on+Spatial+Consistency+Information+for+Semantic+Segmentation%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality](https://arxiv.org/search/?query=%27Characterizing+Adversarial+Subspaces+Using+Local+Intrinsic+Dimensionality%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Characterizing Audio Adversarial Examples Using Temporal Dependency](https://arxiv.org/search/?query=%27Characterizing+Audio+Adversarial+Examples+Using+Temporal+Dependency%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Checkpoint Ensembles: Ensemble Methods from a Single Training Process](https://arxiv.org/search/?query=%27Checkpoint+Ensembles:+Ensemble+Methods+from+a+Single+Training+Process%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Clipping free attacks against artificial neural networks](https://arxiv.org/search/?query=%27Clipping+free+attacks+against+artificial+neural+networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Combinatorial Attacks on Binarized Neural Networks](https://arxiv.org/search/?query=%27Combinatorial+Attacks+on+Binarized+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Comment on Biologically inspired protection of deep networks from adversarial attacks](https://arxiv.org/search/?query=%27Comment+on+Biologically+inspired+protection+of+deep+networks+from+adversarial+attacks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Concolic Testing for Deep Neural Networks](https://arxiv.org/search/?query=%27Concolic+Testing+for+Deep+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Controlling Over-generalization and its Effect on Adversarial Examples Generation and Detection](https://arxiv.org/search/?query=%27Controlling+Over-generalization+and+its+Effect+on+Adversarial+Examples+Generation+and+Detection%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Coverage-Guided Fuzzing for Deep Neural Networks](https://arxiv.org/search/?query=%27Coverage-Guided+Fuzzing+for+Deep+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Crafting Adversarial Examples For Speech Paralinguistics Applications](https://arxiv.org/search/?query=%27Crafting+Adversarial+Examples+For+Speech+Paralinguistics+Applications%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Crafting Adversarial Input Sequences for Recurrent Neural Networks](https://arxiv.org/search/?query=%27Crafting+Adversarial+Input+Sequences+for+Recurrent+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Customizing an Adversarial Example Generator with Class-Conditional GANs](https://arxiv.org/search/?query=%27Customizing+an+Adversarial+Example+Generator+with+Class-Conditional+GANs%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Deep Defense: Training DNNs with Improved Adversarial Robustness](https://arxiv.org/search/?query=%27Deep+Defense:+Training+DNNs+with+Improved+Adversarial+Robustness%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Deep Neural Network Based Malware Detection Using Two Dimensional Binary Program Features](https://arxiv.org/search/?query=%27Deep+Neural+Network+Based+Malware+Detection+Using+Two+Dimensional+Binary+Program+Features%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Deep Text Classification Can be Fooled](https://arxiv.org/search/?query=%27Deep+Text+Classification+Can+be+Fooled%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning](https://arxiv.org/search/?query=%27Deep+k-Nearest+Neighbors:+Towards+Confident,+Interpretable+and+Robust+Deep+Learning%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [DeepBase: Deep Inspection of Neural Networks](https://arxiv.org/search/?query=%27DeepBase:+Deep+Inspection+of+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [DeepCloak: Masking Deep Neural Network Models for Robustness Against Adversarial Samples](https://arxiv.org/search/?query=%27DeepCloak:+Masking+Deep+Neural+Network+Models+for+Robustness+Against+Adversarial+Samples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [DeepFool: a simple and accurate method to fool deep neural networks](https://arxiv.org/search/?query=%27DeepFool:+a+simple+and+accurate+method+to+fool+deep+neural+networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [DeepLaser: Practical Fault Attack on Deep Neural Networks](https://arxiv.org/search/?query=%27DeepLaser:+Practical+Fault+Attack+on+Deep+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [DeepRoad: GAN-based Metamorphic Autonomous Driving System Testing](https://arxiv.org/search/?query=%27DeepRoad:+GAN-based+Metamorphic+Autonomous+Driving+System+Testing%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [DeepSafe: A Data-driven Approach for Checking Adversarial Robustness in Neural Networks](https://arxiv.org/search/?query=%27DeepSafe:+A+Data-driven+Approach+for+Checking+Adversarial+Robustness+in+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [DeepTest: Automated Testing of Deep-Neural-Network-driven Autonomous Cars](https://arxiv.org/search/?query=%27DeepTest:+Automated+Testing+of+Deep-Neural-Network-driven+Autonomous+Cars%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Defend Deep Neural Networks Against Adversarial Examples via Fixed andDynamic Quantized Activation Functions](https://arxiv.org/search/?query=%27Defend+Deep+Neural+Networks+Against+Adversarial+Examples+via+Fixed+andDynamic+Quantized+Activation+Functions%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Defending Malware Classification Networks Against Adversarial Perturbations with Non-Negative Weight Restrictions](https://arxiv.org/search/?query=%27Defending+Malware+Classification+Networks+Against+Adversarial+Perturbations+with+Non-Negative+Weight+Restrictions%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Defense Against Adversarial Attacks with Saak Transform](https://arxiv.org/search/?query=%27Defense+Against+Adversarial+Attacks+with+Saak+Transform%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Defense against Adversarial Attacks Using High-Level Representation Guided Denoiser](https://arxiv.org/search/?query=%27Defense+against+Adversarial+Attacks+Using+High-Level+Representation+Guided+Denoiser%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models](https://arxiv.org/search/?query=%27Defense-GAN:+Protecting+Classifiers+Against+Adversarial+Attacks+Using+Generative+Models%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Defensive Collaborative Multi-task Training - Defending against Adversarial Attack towards Deep Neural Networks](https://arxiv.org/search/?query=%27Defensive+Collaborative+Multi-task+Training+-+Defending+against+Adversarial+Attack+towards+Deep+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Defensive Distillation is Not Robust to Adversarial Examples](https://arxiv.org/search/?query=%27Defensive+Distillation+is+Not+Robust+to+Adversarial+Examples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Defensive Dropout for Hardening Deep Neural Networks under Adversarial Attacks](https://arxiv.org/search/?query=%27Defensive+Dropout+for+Hardening+Deep+Neural+Networks+under+Adversarial+Attacks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Delving into Transferable Adversarial Examples and Black-box Attacks](https://arxiv.org/search/?query=%27Delving+into+Transferable+Adversarial+Examples+and+Black-box+Attacks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Dense Associative Memory is Robust to Adversarial Inputs](https://arxiv.org/search/?query=%27Dense+Associative+Memory+is+Robust+to+Adversarial+Inputs%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Detecting Adversarial Attacks on Neural Network Policies with Visual Foresight](https://arxiv.org/search/?query=%27Detecting+Adversarial+Attacks+on+Neural+Network+Policies+with+Visual+Foresight%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Detecting Adversarial Examples - A Lesson from Multimedia Forensics](https://arxiv.org/search/?query=%27Detecting+Adversarial+Examples+-+A+Lesson+from+Multimedia+Forensics%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Detecting Adversarial Examples via Key-based Network](https://arxiv.org/search/?query=%27Detecting+Adversarial+Examples+via+Key-based+Network%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Detecting Adversarial Examples via Neural Fingerprinting](https://arxiv.org/search/?query=%27Detecting+Adversarial+Examples+via+Neural+Fingerprinting%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Detecting Adversarial Image Examples in Deep Networks with Adaptive Noise Reduction](https://arxiv.org/search/?query=%27Detecting+Adversarial+Image+Examples+in+Deep+Networks+with+Adaptive+Noise+Reduction%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Detecting Adversarial Samples for Deep Neural Networks through Mutation Testing](https://arxiv.org/search/?query=%27Detecting+Adversarial+Samples+for+Deep+Neural+Networks+through+Mutation+Testing%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Detecting Adversarial Samples from Artifacts](https://arxiv.org/search/?query=%27Detecting+Adversarial+Samples+from+Artifacts%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Detection based Defense against Adversarial Examples from the Steganalysis Point ot View](https://arxiv.org/search/?query=%27Detection+based+Defense+against+Adversarial+Examples+from+the+Steganalysis+Point+ot+View%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks](https://arxiv.org/search/?query=%27Distillation+as+a+Defense+to+Adversarial+Perturbations+against+Deep+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Distilling Model Knowledge](https://arxiv.org/search/?query=%27Distilling+Model+Knowledge%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Distilling the Knowledge in a Neural Network](https://arxiv.org/search/?query=%27Distilling+the+Knowledge+in+a+Neural+Network%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Divide, Denoise, and Defend against Adversarial Attacks](https://arxiv.org/search/?query=%27Divide,+Denoise,+and+Defend+against+Adversarial+Attacks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial Examples](https://arxiv.org/search/?query=%27EAD:+Elastic-Net+Attacks+to+Deep+Neural+Networks+via+Adversarial+Examples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [EagleEye: Attack-Agnostic Defense against Adversarial Inputs (Technical Report)](https://arxiv.org/search/?query=%27EagleEye:+Attack-Agnostic+Defense+against+Adversarial+Inputs+(Technical+Report)%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Early Stage Malware Prediction Using Recurrent Neural Networks](https://arxiv.org/search/?query=%27Early+Stage+Malware+Prediction+Using+Recurrent+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Efficient Defenses Against Adversarial Attacks](https://arxiv.org/search/?query=%27Efficient+Defenses+Against+Adversarial+Attacks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Efficient Formal Safety Analysis of Neural Networks](https://arxiv.org/search/?query=%27Efficient+Formal+Safety+Analysis+of+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Efficient Two-Step Adversarial Defense for Deep Neural Networks](https://arxiv.org/search/?query=%27Efficient+Two-Step+Adversarial+Defense+for+Deep+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Enhanced Attacks on Defensively Distilled Deep Neural Networks](https://arxiv.org/search/?query=%27Enhanced+Attacks+on+Defensively+Distilled+Deep+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks](https://arxiv.org/search/?query=%27Ensemble+Methods+as+a+Defense+to+Adversarial+Perturbations+Against+Deep+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Evaluating Robustness of Neural Networks with Mixed Integer Programming](https://arxiv.org/search/?query=%27Evaluating+Robustness+of+Neural+Networks+with+Mixed+Integer+Programming%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Evaluating neural network explanation methods using hybrid documents and morphological agreement](https://arxiv.org/search/?query=%27Evaluating+neural+network+explanation+methods+using+hybrid+documents+and+morphological+agreement%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach](https://arxiv.org/search/?query=%27Evaluating+the+Robustness+of+Neural+Networks:+An+Extreme+Value+Theory+Approach%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Evaluation of Momentum Diverse Input Iterative Fast Gradient Sign Method (M-DI2-FGSM) Based Attack Method on MCS 2018 Adversarial Attacks on Black Box Face Recognition System](https://arxiv.org/search/?query=%27Evaluation+of+Momentum+Diverse+Input+Iterative+Fast+Gradient+Sign+Method+(M-DI2-FGSM)+Based+Attack+Method+on+MCS+2018+Adversarial+Attacks+on+Black+Box+Face+Recognition+System%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Explainable Learning: Implicit Generative Modelling during Training for Adversarial Robustness](https://arxiv.org/search/?query=%27Explainable+Learning:+Implicit+Generative+Modelling+during+Training+for+Adversarial+Robustness%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Explaining and Harnessing Adversarial Examples](https://arxiv.org/search/?query=%27Explaining+and+Harnessing+Adversarial+Examples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Exploring Adversarial Examples in Malware Detection](https://arxiv.org/search/?query=%27Exploring+Adversarial+Examples+in+Malware+Detection%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Exploring the Space of Adversarial Images](https://arxiv.org/search/?query=%27Exploring+the+Space+of+Adversarial+Images%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Exploring the Space of Black-box Attacks on Deep Neural Networks](https://arxiv.org/search/?query=%27Exploring+the+Space+of+Black-box+Attacks+on+Deep+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Extractive Adversarial Networks: High-Recall Explanations for Identifying Personal Attacks in Social Media Posts](https://arxiv.org/search/?query=%27Extractive+Adversarial+Networks:+High-Recall+Explanations+for+Identifying+Personal+Attacks+in+Social+Media+Posts%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Fast Feature Fool: A data independent approach to universal adversarial perturbations](https://arxiv.org/search/?query=%27Fast+Feature+Fool:+A+data+independent+approach+to+universal+adversarial+perturbations%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Feature Distillation: DNN-Oriented JPEG Compression Against Adversarial Examples](https://arxiv.org/search/?query=%27Feature+Distillation:+DNN-Oriented+JPEG+Compression+Against+Adversarial+Examples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks](https://arxiv.org/search/?query=%27Feature+Squeezing:+Detecting+Adversarial+Examples+in+Deep+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Feature-Guided Black-Box Safety Testing of Deep Neural Networks](https://arxiv.org/search/?query=%27Feature-Guided+Black-Box+Safety+Testing+of+Deep+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks](https://arxiv.org/search/?query=%27Fine-Pruning:+Defending+Against+Backdooring+Attacks+on+Deep+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Fooling End-to-end Speaker Verification by Adversarial Examples](https://arxiv.org/search/?query=%27Fooling+End-to-end+Speaker+Verification+by+Adversarial+Examples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Fooling the classifier: Ligand antagonism and adversarial examples](https://arxiv.org/search/?query=%27Fooling+the+classifier:+Ligand+antagonism+and+adversarial+examples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Formal Security Analysis of Neural Networks using Symbolic Intervals](https://arxiv.org/search/?query=%27Formal+Security+Analysis+of+Neural+Networks+using+Symbolic+Intervals%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [FuzzerGym: A Competitive Framework for Fuzzing and Learning](https://arxiv.org/search/?query=%27FuzzerGym:+A+Competitive+Framework+for+Fuzzing+and+Learning%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [GenAttack: Practical Black-box Attacks with Gradient-Free Optimization](https://arxiv.org/search/?query=%27GenAttack:+Practical+Black-box+Attacks+with+Gradient-Free+Optimization%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Generalizable Adversarial Examples Detection Based on Bi-model Decision Mismatch](https://arxiv.org/search/?query=%27Generalizable+Adversarial+Examples+Detection+Based+on+Bi-model+Decision+Mismatch%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Generating 3D Adversarial Point Clouds](https://arxiv.org/search/?query=%27Generating+3D+Adversarial+Point+Clouds%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Generating Adversarial Examples with Adversarial Networks](https://arxiv.org/search/?query=%27Generating+Adversarial+Examples+with+Adversarial+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Generating Natural Language Adversarial Examples](https://arxiv.org/search/?query=%27Generating+Natural+Language+Adversarial+Examples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Generative Adversarial Trainer: Defense to Adversarial Perturbations with GAN](https://arxiv.org/search/?query=%27Generative+Adversarial+Trainer:+Defense+to+Adversarial+Perturbations+with+GAN%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Generative Poisoning Attack Method Against Neural Networks](https://arxiv.org/search/?query=%27Generative+Poisoning+Attack+Method+Against+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Generative adversarial networks and adversarial methods in biomedical image analysis](https://arxiv.org/search/?query=%27Generative+adversarial+networks+and+adversarial+methods+in+biomedical+image+analysis%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Gradient Similarity: An Explainable Approach to Detect Adversarial Attacks against Deep Learning](https://arxiv.org/search/?query=%27Gradient+Similarity:+An+Explainable+Approach+to+Detect+Adversarial+Attacks+against+Deep+Learning%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Graying the black box: Understanding DQNs](https://arxiv.org/search/?query=%27Graying+the+black+box:+Understanding+DQNs%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Hardening Deep Neural Networks via Adversarial Model Cascades](https://arxiv.org/search/?query=%27Hardening+Deep+Neural+Networks+via+Adversarial+Model+Cascades%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Hardware Trojan Attacks on Neural Networks](https://arxiv.org/search/?query=%27Hardware+Trojan+Attacks+on+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [HashTran-DNN: A Framework for Enhancing Robustness of Deep Neural Networks against Adversarial Malware Samples](https://arxiv.org/search/?query=%27HashTran-DNN:+A+Framework+for+Enhancing+Robustness+of+Deep+Neural+Networks+against+Adversarial+Malware+Samples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [HiDDeN: Hiding Data With Deep Networks](https://arxiv.org/search/?query=%27HiDDeN:+Hiding+Data+With+Deep+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [HotFlip: White-Box Adversarial Examples for Text Classification](https://arxiv.org/search/?query=%27HotFlip:+White-Box+Adversarial+Examples+for+Text+Classification%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [How Wrong Am I? - Studying Adversarial Examples and their Impact on Uncertainty in Gaussian Process Machine Learning Models](https://arxiv.org/search/?query=%27How+Wrong+Am+I?+-+Studying+Adversarial+Examples+and+their+Impact+on+Uncertainty+in+Gaussian+Process+Machine+Learning+Models%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Hu-Fu: Hardware and Software Collaborative Attack Framework against Neural Networks](https://arxiv.org/search/?query=%27Hu-Fu:+Hardware+and+Software+Collaborative+Attack+Framework+against+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [HyperNetworks with statistical filtering for defending adversarial examples](https://arxiv.org/search/?query=%27HyperNetworks+with+statistical+filtering+for+defending+adversarial+examples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Improving Back-Propagation by Adding an Adversarial Gradient](https://arxiv.org/search/?query=%27Improving+Back-Propagation+by+Adding+an+Adversarial+Gradient%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Improving DNN Robustness to Adversarial Attacks using Jacobian Regularization](https://arxiv.org/search/?query=%27Improving+DNN+Robustness+to+Adversarial+Attacks+using+Jacobian+Regularization%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Improving Network Robustness against Adversarial Attacks with Compact Convolution](https://arxiv.org/search/?query=%27Improving+Network+Robustness+against+Adversarial+Attacks+with+Compact+Convolution%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Improving Transferability of Adversarial Examples with Input Diversity](https://arxiv.org/search/?query=%27Improving+Transferability+of+Adversarial+Examples+with+Input+Diversity%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing their Input Gradients](https://arxiv.org/search/?query=%27Improving+the+Adversarial+Robustness+and+Interpretability+of+Deep+Neural+Networks+by+Regularizing+their+Input+Gradients%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Interpretable Deep Convolutional Neural Networks via Meta-learning](https://arxiv.org/search/?query=%27Interpretable+Deep+Convolutional+Neural+Networks+via+Meta-learning%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Interpretation of Prediction Models Using the Input Gradient](https://arxiv.org/search/?query=%27Interpretation+of+Prediction+Models+Using+the+Input+Gradient%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Interpreting Adversarial Robustness: A View from Decision Surface in Input Space](https://arxiv.org/search/?query=%27Interpreting+Adversarial+Robustness:+A+View+from+Decision+Surface+in+Input+Space%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Intriguing Properties of Adversarial Examples](https://arxiv.org/search/?query=%27Intriguing+Properties+of+Adversarial+Examples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Is Deep Learning Safe for Robot Vision? Adversarial Examples against the iCub Humanoid](https://arxiv.org/search/?query=%27Is+Deep+Learning+Safe+for+Robot+Vision?+Adversarial+Examples+against+the+iCub+Humanoid%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Isolated and Ensemble Audio Preprocessing Methods for Detecting Adversarial Examples against Automatic Speech Recognition](https://arxiv.org/search/?query=%27Isolated+and+Ensemble+Audio+Preprocessing+Methods+for+Detecting+Adversarial+Examples+against+Automatic+Speech+Recognition%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Keeping the Bad Guys Out: Protecting and Vaccinating Deep Learning with JPEG Compression](https://arxiv.org/search/?query=%27Keeping+the+Bad+Guys+Out:+Protecting+and+Vaccinating+Deep+Learning+with+JPEG+Compression%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [LOTS about Attacking Deep Features](https://arxiv.org/search/?query=%27LOTS+about+Attacking+Deep+Features%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Learning Adversary-Resistant Deep Neural Networks](https://arxiv.org/search/?query=%27Learning+Adversary-Resistant+Deep+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Learning More Robust Features with Adversarial Training](https://arxiv.org/search/?query=%27Learning+More+Robust+Features+with+Adversarial+Training%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Learning Universal Adversarial Perturbations with Generative Models](https://arxiv.org/search/?query=%27Learning+Universal+Adversarial+Perturbations+with+Generative+Models%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Learning with a Strong Adversary](https://arxiv.org/search/?query=%27Learning+with+a+Strong+Adversary%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Less is More: Culling the Training Set to Improve Robustness of Deep Neural Networks](https://arxiv.org/search/?query=%27Less+is+More:+Culling+the+Training+Set+to+Improve+Robustness+of+Deep+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Link Prediction Adversarial Attack](https://arxiv.org/search/?query=%27Link+Prediction+Adversarial+Attack%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Lipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks](https://arxiv.org/search/?query=%27Lipschitz-Margin+Training:+Scalable+Certification+of+Perturbation+Invariance+for+Deep+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Local Gradients Smoothing: Defense against localized adversarial attacks](https://arxiv.org/search/?query=%27Local+Gradients+Smoothing:+Defense+against+localized+adversarial+attacks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [MAT: A Multi-strength Adversarial Training Method to Mitigate Adversarial Attacks](https://arxiv.org/search/?query=%27MAT:+A+Multi-strength+Adversarial+Training+Method+to+Mitigate+Adversarial+Attacks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [MTDeep: Boosting the Security of Deep Neural Nets Against Adversarial Attacks with Moving Target Defense](https://arxiv.org/search/?query=%27MTDeep:+Boosting+the+Security+of+Deep+Neural+Nets+Against+Adversarial+Attacks+with+Moving+Target+Defense%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [MULDEF: Multi-model-based Defense Against Adversarial Examples for Neural Networks](https://arxiv.org/search/?query=%27MULDEF:+Multi-model-based+Defense+Against+Adversarial+Examples+for+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Machine vs Machine: Minimax-Optimal Defense Against Adversarial Examples](https://arxiv.org/search/?query=%27Machine+vs+Machine:+Minimax-Optimal+Defense+Against+Adversarial+Examples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [MagNet: a Two-Pronged Defense against Adversarial Examples](https://arxiv.org/search/?query=%27MagNet:+a+Two-Pronged+Defense+against+Adversarial+Examples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Malytics: A Malware Detection Scheme](https://arxiv.org/search/?query=%27Malytics:+A+Malware+Detection+Scheme%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Manifold Regularized Deep Neural Networks using Adversarial Examples](https://arxiv.org/search/?query=%27Manifold+Regularized+Deep+Neural+Networks+using+Adversarial+Examples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Maximum Resilience of Artificial Neural Networks](https://arxiv.org/search/?query=%27Maximum+Resilience+of+Artificial+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Measuring Neural Net Robustness with Constraints](https://arxiv.org/search/?query=%27Measuring+Neural+Net+Robustness+with+Constraints%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Metric Learning for Novelty and Anomaly Detection](https://arxiv.org/search/?query=%27Metric+Learning+for+Novelty+and+Anomaly+Detection%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Mitigating Adversarial Effects Through Randomization](https://arxiv.org/search/?query=%27Mitigating+Adversarial+Effects+Through+Randomization%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Mitigating Evasion Attacks to Deep Neural Networks via Region-based Classification](https://arxiv.org/search/?query=%27Mitigating+Evasion+Attacks+to+Deep+Neural+Networks+via+Region-based+Classification%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Model compression via distillation and quantization](https://arxiv.org/search/?query=%27Model+compression+via+distillation+and+quantization%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [NEUZZ: Efficient Fuzzing with Neural Program Learning](https://arxiv.org/search/?query=%27NEUZZ:+Efficient+Fuzzing+with+Neural+Program+Learning%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Neural Networks with Structural Resistance to Adversarial Attacks](https://arxiv.org/search/?query=%27Neural+Networks+with+Structural+Resistance+to+Adversarial+Attacks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Neural Stethoscopes: Unifying Analytic, Auxiliary and Adversarial Network Probing](https://arxiv.org/search/?query=%27Neural+Stethoscopes:+Unifying+Analytic,+Auxiliary+and+Adversarial+Network+Probing%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Neural Trojans](https://arxiv.org/search/?query=%27Neural+Trojans%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Non-Negative Networks Against Adversarial Attacks](https://arxiv.org/search/?query=%27Non-Negative+Networks+Against+Adversarial+Attacks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [On Adversarial Examples for Character-Level Neural Machine Translation](https://arxiv.org/search/?query=%27On+Adversarial+Examples+for+Character-Level+Neural+Machine+Translation%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [On Detecting Adversarial Perturbations](https://arxiv.org/search/?query=%27On+Detecting+Adversarial+Perturbations%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [On The Utility of Conditional Generation Based Mutual Information for Characterizing Adversarial Subspaces](https://arxiv.org/search/?query=%27On+The+Utility+of+Conditional+Generation+Based+Mutual+Information+for+Characterizing+Adversarial+Subspaces%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [On the Limitation of Local Intrinsic Dimensionality for Characterizing the Subspaces of Adversarial Examples](https://arxiv.org/search/?query=%27On+the+Limitation+of+Local+Intrinsic+Dimensionality+for+Characterizing+the+Subspaces+of+Adversarial+Examples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [On the Limitation of MagNet Defense against $L_1$-based Adversarial Examples](https://arxiv.org/search/?query=%27On+the+Limitation+of+MagNet+Defense+against+$L_1$-based+Adversarial+Examples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [On the Robustness of Semantic Segmentation Models to Adversarial Attacks](https://arxiv.org/search/?query=%27On+the+Robustness+of+Semantic+Segmentation+Models+to+Adversarial+Attacks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [On the Robustness of the CVPR 2018 White-Box Adversarial Example Defenses](https://arxiv.org/search/?query=%27On+the+Robustness+of+the+CVPR+2018+White-Box+Adversarial+Example+Defenses%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [On the Utility of Conditional Generation Based Mutual Information for Characterizing Adversarial Subspaces](https://arxiv.org/search/?query=%27On+the+Utility+of+Conditional+Generation+Based+Mutual+Information+for+Characterizing+Adversarial+Subspaces%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [One pixel attack for fooling deep neural networks](https://arxiv.org/search/?query=%27One+pixel+attack+for+fooling+deep+neural+networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Out-distribution training confers robustness to deep neural networks](https://arxiv.org/search/?query=%27Out-distribution+training+confers+robustness+to+deep+neural+networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Output Range Analysis for Deep Neural Networks](https://arxiv.org/search/?query=%27Output+Range+Analysis+for+Deep+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [PRADA: Protecting against DNN Model Stealing Attacks](https://arxiv.org/search/?query=%27PRADA:+Protecting+against+DNN+Model+Stealing+Attacks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [PeerNets: Exploiting Peer Wisdom Against Adversarial Attacks](https://arxiv.org/search/?query=%27PeerNets:+Exploiting+Peer+Wisdom+Against+Adversarial+Attacks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples](https://arxiv.org/search/?query=%27PixelDefend:+Leveraging+Generative+Models+to+Understand+and+Defend+against+Adversarial+Examples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [PoTrojan: powerful neural-level trojan designs in deep learning models](https://arxiv.org/search/?query=%27PoTrojan:+powerful+neural-level+trojan+designs+in+deep+learning+models%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks](https://arxiv.org/search/?query=%27Poison+Frogs!+Targeted+Clean-Label+Poisoning+Attacks+on+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Practical Black-Box Attacks against Machine Learning](https://arxiv.org/search/?query=%27Practical+Black-Box+Attacks+against+Machine+Learning%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Procedural Noise Adversarial Examples for Black-Box Attacks on Deep Neural Networks](https://arxiv.org/search/?query=%27Procedural+Noise+Adversarial+Examples+for+Black-Box+Attacks+on+Deep+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Protecting JPEG Images Against Adversarial Attacks](https://arxiv.org/search/?query=%27Protecting+JPEG+Images+Against+Adversarial+Attacks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Provably Minimally-Distorted Adversarial Examples](https://arxiv.org/search/?query=%27Provably+Minimally-Distorted+Adversarial+Examples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Query-Efficient Black-Box Attack by Active Learning](https://arxiv.org/search/?query=%27Query-Efficient+Black-Box+Attack+by+Active+Learning%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Query-Efficient Black-box Adversarial Examples (superceded)](https://arxiv.org/search/?query=%27Query-Efficient+Black-box+Adversarial+Examples+(superceded)%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Query-Free Attacks on Industry-Grade Face Recognition Systems under Resource Constraints](https://arxiv.org/search/?query=%27Query-Free+Attacks+on+Industry-Grade+Face+Recognition+Systems+under+Resource+Constraints%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [ROPNN: Detection of ROP Payloads Using Deep Neural Networks](https://arxiv.org/search/?query=%27ROPNN:+Detection+of+ROP+Payloads+Using+Deep+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [ReabsNet: Detecting and Revising Adversarial Examples](https://arxiv.org/search/?query=%27ReabsNet:+Detecting+and+Revising+Adversarial+Examples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Recurrent Neural Network Attention Mechanisms for Interpretable System Log Anomaly Detection](https://arxiv.org/search/?query=%27Recurrent+Neural+Network+Attention+Mechanisms+for+Interpretable+System+Log+Anomaly+Detection%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Regularizing deep networks using efficient layerwise adversarial training](https://arxiv.org/search/?query=%27Regularizing+deep+networks+using+efficient+layerwise+adversarial+training%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks](https://arxiv.org/search/?query=%27Reluplex:+An+Efficient+SMT+Solver+for+Verifying+Deep+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Resisting Adversarial Attacks using Gaussian Mixture Variational Autoencoders](https://arxiv.org/search/?query=%27Resisting+Adversarial+Attacks+using+Gaussian+Mixture+Variational+Autoencoders%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Rethinking generalization requires revisiting old ideas: statistical mechanics approaches and complex learning behavior](https://arxiv.org/search/?query=%27Rethinking+generalization+requires+revisiting+old+ideas:+statistical+mechanics+approaches+and+complex+learning+behavior%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Retrieval-Augmented Convolutional Neural Networks for Improved Robustness against Adversarial Examples](https://arxiv.org/search/?query=%27Retrieval-Augmented+Convolutional+Neural+Networks+for+Improved+Robustness+against+Adversarial+Examples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Revisiting Batch Normalization For Practical Domain Adaptation](https://arxiv.org/search/?query=%27Revisiting+Batch+Normalization+For+Practical+Domain+Adaptation%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Robust Convolutional Neural Networks under Adversarial Noise](https://arxiv.org/search/?query=%27Robust+Convolutional+Neural+Networks+under+Adversarial+Noise%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Robust Physical-World Attacks on Deep Learning Models](https://arxiv.org/search/?query=%27Robust+Physical-World+Attacks+on+Deep+Learning+Models%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Robustness of Rotation-Equivariant Networks to Adversarial Perturbations](https://arxiv.org/search/?query=%27Robustness+of+Rotation-Equivariant+Networks+to+Adversarial+Perturbations%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Safety Verification of Deep Neural Networks](https://arxiv.org/search/?query=%27Safety+Verification+of+Deep+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with Adversarial Examples](https://arxiv.org/search/?query=%27Seq2Sick:+Evaluating+the+Robustness+of+Sequence-to-Sequence+Models+with+Adversarial+Examples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Sequential Attacks on Agents for Long-Term Adversarial Goals](https://arxiv.org/search/?query=%27Sequential+Attacks+on+Agents+for+Long-Term+Adversarial+Goals%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [ShapeShifter: Robust Physical Adversarial Attack on Faster R-CNN Object Detector](https://arxiv.org/search/?query=%27ShapeShifter:+Robust+Physical+Adversarial+Attack+on+Faster+R-CNN+Object+Detector%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Shield: Fast, Practical Defense and Vaccination for Deep Learning using JPEG Compression](https://arxiv.org/search/?query=%27Shield:+Fast,+Practical+Defense+and+Vaccination+for+Deep+Learning+using+JPEG+Compression%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Siamese Generative Adversarial Privatizer for Biometric Data](https://arxiv.org/search/?query=%27Siamese+Generative+Adversarial+Privatizer+for+Biometric+Data%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Simple Black-Box Adversarial Perturbations for Deep Networks](https://arxiv.org/search/?query=%27Simple+Black-Box+Adversarial+Perturbations+for+Deep+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Simulation-based Adversarial Test Generation for Autonomous Vehicles with Machine Learning Components](https://arxiv.org/search/?query=%27Simulation-based+Adversarial+Test+Generation+for+Autonomous+Vehicles+with+Machine+Learning+Components%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Simultaneous Adversarial Training - Learn from Others Mistakes](https://arxiv.org/search/?query=%27Simultaneous+Adversarial+Training+-+Learn+from+Others+Mistakes%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Sparse DNNs with Improved Adversarial Robustness](https://arxiv.org/search/?query=%27Sparse+DNNs+with+Improved+Adversarial+Robustness%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Sparsity-based Defense against Adversarial Attacks on Linear Classifiers](https://arxiv.org/search/?query=%27Sparsity-based+Defense+against+Adversarial+Attacks+on+Linear+Classifiers%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Spatially Transformed Adversarial Examples](https://arxiv.org/search/?query=%27Spatially+Transformed+Adversarial+Examples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Statistical mechanics of unsupervised feature learning in a restricted Boltzmann machine with binary synapses](https://arxiv.org/search/?query=%27Statistical+mechanics+of+unsupervised+feature+learning+in+a+restricted+Boltzmann+machine+with+binary+synapses%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Stochastic Activation Pruning for Robust Adversarial Defense](https://arxiv.org/search/?query=%27Stochastic+Activation+Pruning+for+Robust+Adversarial+Defense%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Stochastic Combinatorial Ensembles for Defending Against Adversarial Examples](https://arxiv.org/search/?query=%27Stochastic+Combinatorial+Ensembles+for+Defending+Against+Adversarial+Examples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Stochastic Substitute Training: A Gray-box Approach to Craft Adversarial Examples Against Gradient Obfuscation Defenses](https://arxiv.org/search/?query=%27Stochastic+Substitute+Training:+A+Gray-box+Approach+to+Craft+Adversarial+Examples+Against+Gradient+Obfuscation+Defenses%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Structured Adversarial Attack: Towards General Implementation and Better Interpretability](https://arxiv.org/search/?query=%27Structured+Adversarial+Attack:+Towards+General+Implementation+and+Better+Interpretability%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Sufficient Conditions for Idealised Models to Have No Adversarial Examples: a Theoretical and Empirical Study with Bayesian Neural Networks](https://arxiv.org/search/?query=%27Sufficient+Conditions+for+Idealised+Models+to+Have+No+Adversarial+Examples:+a+Theoretical+and+Empirical+Study+with+Bayesian+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Suppressing the Unusual: towards Robust CNNs using Symmetric Activation Functions](https://arxiv.org/search/?query=%27Suppressing+the+Unusual:+towards+Robust+CNNs+using+Symmetric+Activation+Functions%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Symbolic Execution for Deep Neural Networks](https://arxiv.org/search/?query=%27Symbolic+Execution+for+Deep+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Synthesizing Deep Neural Network Architectures using Biological Synaptic Strength Distributions](https://arxiv.org/search/?query=%27Synthesizing+Deep+Neural+Network+Architectures+using+Biological+Synaptic+Strength+Distributions%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Synthesizing Robust Adversarial Examples](https://arxiv.org/search/?query=%27Synthesizing+Robust+Adversarial+Examples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Targeted Adversarial Examples for Black Box Audio Systems](https://arxiv.org/search/?query=%27Targeted+Adversarial+Examples+for+Black+Box+Audio+Systems%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing](https://arxiv.org/search/?query=%27TensorFuzz:+Debugging+Neural+Networks+with+Coverage-Guided+Fuzzing%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Testing Deep Neural Networks](https://arxiv.org/search/?query=%27Testing+Deep+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [The Limitations of Deep Learning in Adversarial Settings](https://arxiv.org/search/?query=%27The+Limitations+of+Deep+Learning+in+Adversarial+Settings%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [The Robust Manifold Defense: Adversarial Training using Generative Models](https://arxiv.org/search/?query=%27The+Robust+Manifold+Defense:+Adversarial+Training+using+Generative+Models%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [To compress or not to compress: Understanding the Interactions between Adversarial Attacks and Neural Network Compression](https://arxiv.org/search/?query=%27To+compress+or+not+to+compress:+Understanding+the+Interactions+between+Adversarial+Attacks+and+Neural+Network+Compression%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Toward Robust Neural Networks via Sparsification](https://arxiv.org/search/?query=%27Toward+Robust+Neural+Networks+via+Sparsification%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Towards Adversarial Training with Moderate Performance Improvement for Neural Network Classification](https://arxiv.org/search/?query=%27Towards+Adversarial+Training+with+Moderate+Performance+Improvement+for+Neural+Network+Classification%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Towards Deep Learning Models Resistant to Adversarial Attacks](https://arxiv.org/search/?query=%27Towards+Deep+Learning+Models+Resistant+to+Adversarial+Attacks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Towards Deep Neural Network Architectures Robust to Adversarial Examples](https://arxiv.org/search/?query=%27Towards+Deep+Neural+Network+Architectures+Robust+to+Adversarial+Examples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Towards Dependable Deep Convolutional Neural Networks (CNNs) with Out-distribution Learning](https://arxiv.org/search/?query=%27Towards+Dependable+Deep+Convolutional+Neural+Networks+(CNNs)+with+Out-distribution+Learning%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Towards Evaluating the Robustness of Neural Networks](https://arxiv.org/search/?query=%27Towards+Evaluating+the+Robustness+of+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Towards Imperceptible and Robust Adversarial Example Attacks against Neural Networks](https://arxiv.org/search/?query=%27Towards+Imperceptible+and+Robust+Adversarial+Example+Attacks+against+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Towards Poisoning of Deep Learning Algorithms with Back-gradient Optimization](https://arxiv.org/search/?query=%27Towards+Poisoning+of+Deep+Learning+Algorithms+with+Back-gradient+Optimization%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Towards Practical Verification of Machine Learning: The Case of Computer Vision Systems](https://arxiv.org/search/?query=%27Towards+Practical+Verification+of+Machine+Learning:+The+Case+of+Computer+Vision+Systems%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Towards Query Efficient Black-box Attacks: An Input-free Perspective](https://arxiv.org/search/?query=%27Towards+Query+Efficient+Black-box+Attacks:+An+Input-free+Perspective%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Towards Reverse-Engineering Black-Box Neural Networks](https://arxiv.org/search/?query=%27Towards+Reverse-Engineering+Black-Box+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Towards Robust Deep Neural Networks with BANG](https://arxiv.org/search/?query=%27Towards+Robust+Deep+Neural+Networks+with+BANG%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Towards Robust Neural Networks via Random Self-ensemble](https://arxiv.org/search/?query=%27Towards+Robust+Neural+Networks+via+Random+Self-ensemble%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Towards Robust Training of Neural Networks by Regularizing Adversarial Gradients](https://arxiv.org/search/?query=%27Towards+Robust+Training+of+Neural+Networks+by+Regularizing+Adversarial+Gradients%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Towards the first adversarially robust neural network model on MNIST](https://arxiv.org/search/?query=%27Towards+the+first+adversarially+robust+neural+network+model+on+MNIST%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Training Augmentation with Adversarial Examples for Robust Speech Recognition](https://arxiv.org/search/?query=%27Training+Augmentation+with+Adversarial+Examples+for+Robust+Speech+Recognition%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Training for Faster Adversarial Robustness Verification via Inducing ReLU Stability](https://arxiv.org/search/?query=%27Training+for+Faster+Adversarial+Robustness+Verification+via+Inducing+ReLU+Stability%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Traits and Transferability of Adversarial Examples against Instance Segmentation and Object Detection](https://arxiv.org/search/?query=%27Traits+and+Transferability+of+Adversarial+Examples+against+Instance+Segmentation+and+Object+Detection%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Transfer Learning for Voice Activity Detection: A Denoising Deep Neural Network Perspective](https://arxiv.org/search/?query=%27Transfer+Learning+for+Voice+Activity+Detection:+A+Denoising+Deep+Neural+Network+Perspective%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring](https://arxiv.org/search/?query=%27Turning+Your+Weakness+Into+a+Strength:+Watermarking+Deep+Neural+Networks+by+Backdooring%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Understanding Adversarial Training: Increasing Local Stability of Neural Nets through Robust Optimization](https://arxiv.org/search/?query=%27Understanding+Adversarial+Training:+Increasing+Local+Stability+of+Neural+Nets+through+Robust+Optimization%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Understanding and Enhancing the Transferability of Adversarial Examples](https://arxiv.org/search/?query=%27Understanding+and+Enhancing+the+Transferability+of+Adversarial+Examples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Unifying Bilateral Filtering and Adversarial Training for Robust Neural Networks](https://arxiv.org/search/?query=%27Unifying+Bilateral+Filtering+and+Adversarial+Training+for+Robust+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Universal adversarial perturbations](https://arxiv.org/search/?query=%27Universal+adversarial+perturbations%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Unravelling Robustness of Deep Learning based Face Recognition Against Adversarial Attacks](https://arxiv.org/search/?query=%27Unravelling+Robustness+of+Deep+Learning+based+Face+Recognition+Against+Adversarial+Attacks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Using Non-invertible Data Transformations to Build Adversarial-Robust Neural Networks](https://arxiv.org/search/?query=%27Using+Non-invertible+Data+Transformations+to+Build+Adversarial-Robust+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [VectorDefense: Vectorization as a Defense to Adversarial Examples](https://arxiv.org/search/?query=%27VectorDefense:+Vectorization+as+a+Defense+to+Adversarial+Examples%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Verification of Binarized Neural Networks via Inter-Neuron Factoring](https://arxiv.org/search/?query=%27Verification+of+Binarized+Neural+Networks+via+Inter-Neuron+Factoring%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Verifying Properties of Binarized Deep Neural Networks](https://arxiv.org/search/?query=%27Verifying+Properties+of+Binarized+Deep+Neural+Networks%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [When Not to Classify: Anomaly Detection of Attacks (ADA) on DNN Classifiers at Test Time](https://arxiv.org/search/?query=%27When+Not+to+Classify:+Anomaly+Detection+of+Attacks+(ADA)+on+DNN+Classifiers+at+Test+Time%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [Where Classification Fails, Interpretation Rises](https://arxiv.org/search/?query=%27Where+Classification+Fails,+Interpretation+Rises%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [With Friends Like These, Who Needs Adversaries](https://arxiv.org/search/?query=%27With+Friends+Like+These,+Who+Needs+Adversaries%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [ZOO: Zeroth Order Optimization based Black-box Attacks to Deep Neural Networks without Training Substitute Models](https://arxiv.org/search/?query=%27ZOO:+Zeroth+Order+Optimization+based+Black-box+Attacks+to+Deep+Neural+Networks+without+Training+Substitute+Models%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [mixup: Beyond Empirical Risk Minimization](https://arxiv.org/search/?query=%27mixup:+Beyond+Empirical+Risk+Minimization%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
1. [zoNNscan : a boundary-entropy index for zone inspection of neural models](https://arxiv.org/search/?query=%27zoNNscan+:+a+boundary-entropy+index+for+zone+inspection+of+neural+models%27&searchtype=title&abstracts=show&order=-announced_date_first&size=50)
